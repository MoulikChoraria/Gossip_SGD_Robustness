{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "graph_struct.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKqF1_zedfOc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "6fa4bdfa-8e03-4d48-d09a-c7d73b48fb93"
      },
      "source": [
        "import autoreload\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f1203a6ea90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt9hgbwZdfO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Node():\n",
        "    \"\"\"Node(Choco_Gossip): x_i(t+1) = x_i(t) + gamma*Sum(w_ij*[xhat_j(t+1) - xhat_i(t+1)])\"\"\"\n",
        "    \n",
        "    def __init__(self, gamma, loader, model, criterion):\n",
        "        \n",
        "        self.neighbors = []\n",
        "        self.neighbor_wts = {}\n",
        "        \n",
        "        self.step_size = gamma\n",
        "                \n",
        "        self.dataloader = loader\n",
        "        \n",
        "        self.model = model\n",
        "        \n",
        "        self.x_i = OrderedDict()\n",
        "        \n",
        "        self.model_params = []\n",
        "        for (k,v) in self.model.state_dict().items():\n",
        "            \n",
        "            self.model_params.append(k)\n",
        "            self.x_i[k] = v.clone().detach()\n",
        "            \n",
        "        #for a in self.model.parameters():\n",
        "        #    self.x_i.append(a)\n",
        "        \n",
        "        self.criterion = criterion\n",
        "        \n",
        "        self.dataiter = iter(self.dataloader)\n",
        "        \n",
        "    \n",
        "    def compute_gradient(self, quantizer=None, ):\n",
        "        \"\"\"Computes nabla(x_i, samples) and returns estimate after quantization\"\"\"\n",
        "        \n",
        "        # Sample batch from loader #\n",
        "        optimizer  = optim.SGD(self.model.parameters(), lr=1e-3)\n",
        "        #for v in self.model.parameters():\n",
        "        #  if v.grad is not None:\n",
        "        #    v.detach_()\n",
        "        #    v.zero_()\n",
        "\n",
        "        optimizer.zero_grad()    \n",
        "        inputs, targets = self.dataiter.next()\n",
        "        \n",
        "        outputs = self.model(inputs)\n",
        "\n",
        "\n",
        "        loss = self.criterion(outputs, targets)\n",
        "        \n",
        "        #Equivalent to optimizer.zero_grad()\n",
        "        \n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        gt = OrderedDict()\n",
        "        \n",
        "        \n",
        "        for k,v in enumerate(self.model.parameters()):\n",
        "            if v.grad is not None:\n",
        "                if quantizer is not None:\n",
        "                    gt[k] = quantizer(v.grad.clone().detach_())\n",
        "                else:\n",
        "                    gt[k] = v.grad.clone().detach()\n",
        "        #optimizer.step()\n",
        "    \n",
        "        self.curr_gt = gt\n",
        "        \n",
        "        return\n",
        "    \n",
        "    def assign_params(self, W):\n",
        "        \"\"\"Assign dict W to model\"\"\"\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            self.model.load_state_dict(W, strict=False)\n",
        "        \n",
        "        return\n",
        "    \n",
        "    def update_model(self):\n",
        "        \n",
        "        ### Implement Algorithm ###\n",
        "        \n",
        "        ## Assign Parameters after obtaining Consensus##\n",
        "        \n",
        "        \n",
        "        self.assign_params(self.x_i)\n",
        "        \n",
        "        return        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja2ViNMhdfPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network():\n",
        "    \"\"\"Define graph\"\"\"\n",
        "    \n",
        "    def __init__(self, W, models, learning_rates, loaders, criterion):\n",
        "        \n",
        "        self.adjacency = W\n",
        "        self.num_nodes = W.shape[0]\n",
        "        \n",
        "        self.nodes = OrderedDict()\n",
        "        \n",
        "        for i in range(self.num_nodes):\n",
        "            self.nodes[str(i)] = Node(learning_rates[i], loaders[i],models[i], criterion)\n",
        "            for j in range(self.num_nodes):\n",
        "                if(j != i and W[i, j] > 0):\n",
        "                    self.nodes[str(i)].neighbors.append(j)\n",
        "                    self.nodes[str(i)].neighbor_wts[str(j)] = W[i, j]\n",
        "                    \n",
        "            \n",
        "    def simulate(self, iterations, epochs):\n",
        "        \n",
        "        for i in range(epochs):\n",
        "            for j in range(iterations):\n",
        "                lr = 1e-3\n",
        "                print(j)\n",
        "                for k in range(self.num_nodes):\n",
        "                    self.nodes[str(k)].compute_gradient()\n",
        "                \n",
        "                \n",
        "                for l in range(self.num_nodes):\n",
        "                    for m,param in enumerate(self.nodes[str(l)].model.parameters()):\n",
        "                        if param.grad is None:\n",
        "                            continue\n",
        "                      \n",
        "                        gt_update = self.nodes[str(l)].curr_gt[m]\n",
        "                        wt_sum = 1\n",
        "                        for n in self.nodes[str(l)].neighbors:\n",
        "                            gt_update= gt_update + self.nodes[str(l)].neighbor_wts[str(n)] *self.nodes[str(n)].curr_gt[m]\n",
        "                            wt_sum = wt_sum + self.nodes[str(l)].neighbor_wts[str(n)]\n",
        "                        gt_update = gt_update/wt_sum\n",
        "                        param.data = param.data - lr*gt_update\n",
        "                    #self.nodes[str(l)].update_model()    \n",
        "                        \n",
        "                        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evtLbRBYdfPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "fbd31228-13c6-42d3-ad1c-5b3fd09310fe"
      },
      "source": [
        "models = [torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True) for i in range(3)]\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloaders = [torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2) for i in range(3)]\n",
        "net = Network(torch.ones([3,3]),models, [1e-3,1e-3,1e-3],trainloaders,nn.CrossEntropyLoss())"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n",
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnUz2KASgPZb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = Network(torch.ones([3,3]),models, [1e-3,1e-3,1e-3],trainloaders,nn.CrossEntropyLoss())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3AaQYUNdfPi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "c760edfd-314b-42fb-f891-d300466829e1"
      },
      "source": [
        "net.simulate(10,1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7s7Kq-adfPv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a75f78ca-aef7-438e-efc9-4d8d00a105ae"
      },
      "source": [
        "net.nodes['0'].model"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaKxZ97ndfP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}